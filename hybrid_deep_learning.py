# -*- coding: utf-8 -*-
"""Hybrid_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KmJbSpYf6qjNv-24oVm3n_FPZSPRvN_U

#Unsupervised Deep Learning (SOM)
"""

!pip install MiniSom

"""##Importing the necessary libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from minisom import MiniSom
from pylab import bone, pcolor, colorbar, plot, show,figure
import seaborn as sns

"""## Part 1 - Data Preprocessing"""

df = pd.read_csv('Credit_Card_Applications.csv')
df_f = df.copy()

df.head()

"""Last column represents the clients appliance status. 1 is for Applied 0 is for not applied

Last column need to be dropped
"""

y = df.iloc[:, -1].values
df = pd.DataFrame(df.drop(["Class"], axis=1))

df.shape

df.isna().any()

"""###cols"""

def col_types(dataframe, cat_th=10, car_th=30):
    """

    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.
    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.

    Parameters
    ------
        dataframe: dataframe
                Dataframe for isnpection
        cat_th: int, optional
                categorical columns that seem like numerical
        car_th: int, optinal
                cardinal columns that seem like categorical

    Returns
    ------
        cat_cols: list
                categorical column names list
        num_cols: list
                numerical column names list
        cat_but_car: list
                cardinal column names list

    Examples
    ------
        import seaborn as sns
        df = sns.load_dataset("iris")
        print(grab_col_names(df))


    Notes
    ------
        cat_cols + num_cols + cat_but_car = toplam değişken sayısı


    """

    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')
    print(f'num_cols: {len(num_cols)}')
    print(f'cat_but_car: {len(cat_but_car)}')
    print(f'num_but_cat: {len(num_but_cat)}')
    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = col_types(df)

"""### Correlations"""

def correlation_matrix(df, cols):
    fig = plt.gcf()
    fig.set_size_inches(10, 8)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='RdBu')
    plt.show(block=True)
correlation_matrix(df,num_cols)

"""Missing Column check"""

def missing_col(dataframe, na_name=False):
    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]

    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)
    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)
    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])
    print("Missing columns")
    print(missing_df, end="\n")

    if na_name:
        return na_columns
missing_col(df)

"""Using MinMaxScaler instead of normalizing because the distribution of the quantity is normal"""

sc = MinMaxScaler(feature_range = (0,1))
df = sc.fit_transform(df)

"""##Training SOM """

som = MiniSom(x=10, y=10, input_len= 15)
som.random_weights_init(df)
som.train_random(data = df, num_iteration = 100)

"""Creating  Self Organizing Map by using MiniSom

## Visualizing the map to determine potential frauds
"""

bone()
pcolor(som.distance_map().T)
colorbar()
markers = ['o', 's']
colors = ['b', 'g']
for i, x in enumerate(df):
    w = som.winner(x)
    plot(w[0] + 0.5,
         w[1] + 0.5,
         markers[y[i]],
         markeredgecolor = colors[y[i]],
         markerfacecolor = 'None',
         markersize = 10,
         markeredgewidth = 2)
show()

"""Potential cheaters have high MID value (White)

## Finding the frauds
"""

mappings = som.win_map(df)

frauds = np.concatenate((mappings[(5,7)], mappings[(7,6)],mappings[(4,1)]), axis = 0)
frauds = sc.inverse_transform(frauds)

"""#Supervised Deep Learning (ANN)

##Create Matrix of Features
"""

df = pd.DataFrame(df)

all_customers = df.iloc[:, 1:].values

"""## Create Dependent Variable"""

is_fraud = np.zeros(len(df))
for i in range(len(df)):
  if df.iloc[i,0] in frauds:
    is_fraud[i] = 1

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
all_customers = sc.fit_transform(all_customers)

"""##Tensorflow"""

import tensorflow as tf
tf.__version__

"""## Building the ANN model"""

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=2, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

ann.fit(all_customers, is_fraud, batch_size = 1, epochs = 10)

"""## Predicting test set results"""

y_pred = ann.predict(all_customers)
y_pred = np.concatenate((df_f.iloc[:, 0:1].values, y_pred), axis = 1)
y_pred = y_pred[y_pred[:, 1].argsort()]
y_pred = pd.DataFrame(y_pred)
y_pred.columns = ["Customer ID","Probability"]
print(y_pred)